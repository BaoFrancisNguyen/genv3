"""
Service d'Export - Couche m√©tier pour l'export de donn√©es
========================================================

Ce service g√®re l'export des donn√©es g√©n√©r√©es vers diff√©rents formats
avec validation, optimisation et gestion des erreurs de haut niveau.
"""

import logging
import pandas as pd
from datetime import datetime
from typing import Dict, List, Optional
import os

from src.core.data_exporter import DataExporter
from config import EXPORT_CONFIG, APP_CONFIG


# Configuration du logger
logger = logging.getLogger(__name__)


class ExportService:
    """
    Service m√©tier pour l'export de donn√©es
    
    Orchestre l'export des donn√©es en g√©rant la validation,
    la conversion et l'optimisation selon les besoins m√©tier.
    """
    
    def __init__(self, data_exporter: DataExporter):
        """
        Initialise le service d'export
        
        Args:
            data_exporter: Instance de l'exporteur de donn√©es
        """
        self.data_exporter = data_exporter
        self.service_statistics = {
            'total_exports': 0,
            'successful_exports': 0,
            'total_files_created': 0,
            'total_size_exported_mb': 0.0,
            'service_start_time': datetime.now()
        }
        
        logger.info("‚úÖ Service d'export initialis√©")
    
    def export_complete_dataset(
        self,
        buildings_data: List[Dict],
        timeseries_data: List[Dict],
        formats: List[str] = None,
        filename_prefix: str = None,
        export_options: Dict = None
    ) -> Dict:
        """
        Exporte un dataset complet avec optimisations m√©tier
        
        Args:
            buildings_data: Donn√©es des b√¢timents
            timeseries_data: Donn√©es des s√©ries temporelles
            formats: Liste des formats √† exporter
            filename_prefix: Pr√©fixe des fichiers
            export_options: Options d'export personnalis√©es
            
        Returns:
            Dict: R√©sultats d'export avec m√©tadonn√©es
        """
        try:
            logger.info(f"üîÑ Export dataset complet - {len(buildings_data)} b√¢timents, {len(timeseries_data)} observations")
            
            # Mise √† jour des statistiques
            self.service_statistics['total_exports'] += 1
            
            # Phase 1: Validation et pr√©paration des donn√©es
            preparation_result = self._prepare_data_for_export(
                buildings_data, timeseries_data, export_options
            )
            
            if not preparation_result['success']:
                return preparation_result
            
            buildings_df = preparation_result['buildings_df']
            timeseries_df = preparation_result['timeseries_df']
            
            # Phase 2: Optimisation selon les formats demand√©s
            if formats is None:
                formats = ['csv', 'parquet']  # Formats par d√©faut
            
            optimized_formats = self._optimize_export_formats(formats, len(timeseries_data))
            
            # Phase 3: G√©n√©ration du pr√©fixe de fichier intelligent
            if filename_prefix is None:
                filename_prefix = self._generate_intelligent_filename(
                    buildings_data, timeseries_data
                )
            
            # Phase 4: Export via l'exporteur de base
            export_result = self.data_exporter.export_complete_dataset(
                buildings_df=buildings_df,
                timeseries_df=timeseries_df,
                formats=optimized_formats,
                filename_prefix=filename_prefix
            )
            
            # Phase 5: Post-traitement et enrichissement
            if export_result['success']:
                self.service_statistics['successful_exports'] += 1
                self.service_statistics['total_size_exported_mb'] += export_result['total_size_mb']
                
                # Cr√©ation du manifest enrichi
                manifest_path = self.data_exporter.create_export_manifest(export_result)
                
                # Enrichissement avec m√©tadonn√©es m√©tier
                enriched_result = self._enrich_export_result(
                    export_result, preparation_result, manifest_path
                )
                
                logger.info(f"‚úÖ Export r√©ussi: {export_result['total_size_mb']:.2f} MB")
                return enriched_result
            else:
                return export_result
                
        except Exception as e:
            logger.error(f"‚ùå Erreur service export: {str(e)}")
            return {
                'success': False,
                'error': f"Erreur service d'export: {str(e)}"
            }
    
    def _prepare_data_for_export(
        self,
        buildings_data: List[Dict],
        timeseries_data: List[Dict],
        export_options: Dict = None
    ) -> Dict:
        """
        Pr√©pare et valide les donn√©es pour l'export
        
        Args:
            buildings_data: Donn√©es des b√¢timents
            timeseries_data: Donn√©es des s√©ries temporelles
            export_options: Options d'export
            
        Returns:
            Dict: Donn√©es pr√©par√©es et valid√©es
        """
        try:
            if export_options is None:
                export_options = {}
            
            # Validation de base
            if not buildings_data:
                return {
                    'success': False,
                    'error': 'Aucune donn√©e de b√¢timents √† exporter'
                }
            
            if not timeseries_data:
                return {
                    'success': False,
                    'error': 'Aucune donn√©e de s√©ries temporelles √† exporter'
                }
            
            # Conversion en DataFrames avec optimisations
            buildings_df = self._convert_buildings_to_dataframe(buildings_data, export_options)
            timeseries_df = self._convert_timeseries_to_dataframe(timeseries_data, export_options)
            
            # Validation de coh√©rence
            coherence_check = self._check_data_coherence(buildings_df, timeseries_df)
            
            if not coherence_check['coherent']:
                logger.warning(f"‚ö†Ô∏è Probl√®mes de coh√©rence d√©tect√©s: {coherence_check['issues']}")
            
            # Application des filtres et optimisations
            if export_options.get('apply_filters', True):
                buildings_df = self._apply_data_filters(buildings_df, 'buildings')
                timeseries_df = self._apply_data_filters(timeseries_df, 'timeseries')
            
            return {
                'success': True,
                'buildings_df': buildings_df,
                'timeseries_df': timeseries_df,
                'preparation_info': {
                    'original_buildings_count': len(buildings_data),
                    'original_timeseries_count': len(timeseries_data),
                    'final_buildings_count': len(buildings_df),
                    'final_timeseries_count': len(timeseries_df),
                    'coherence_check': coherence_check
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©paration donn√©es: {str(e)}")
            return {
                'success': False,
                'error': f"Erreur pr√©paration: {str(e)}"
            }
    
    def _convert_buildings_to_dataframe(self, buildings_data: List[Dict], options: Dict) -> pd.DataFrame:
        """
        Convertit les donn√©es de b√¢timents en DataFrame optimis√©
        
        Args:
            buildings_data: Donn√©es des b√¢timents
            options: Options de conversion
            
        Returns:
            pd.DataFrame: DataFrame optimis√©
        """
        df = pd.DataFrame(buildings_data)
        
        # Optimisations des types de donn√©es
        if 'building_type' in df.columns:
            df['building_type'] = df['building_type'].astype('category')
        
        if 'zone_name' in df.columns:
            df['zone_name'] = df['zone_name'].astype('category')
        
        # Arrondissement des coordonn√©es
        if 'latitude' in df.columns:
            df['latitude'] = df['latitude'].round(6)
        if 'longitude' in df.columns:
            df['longitude'] = df['longitude'].round(6)
        
        # Arrondissement des valeurs num√©riques
        numeric_cols = df.select_dtypes(include=['float64', 'float32']).columns
        for col in numeric_cols:
            if 'consumption' in col.lower():
                df[col] = df[col].round(4)
            elif col in ['surface_area_m2']:
                df[col] = df[col].round(1)
        
        # Tri pour optimiser la compression
        if 'zone_name' in df.columns and 'building_type' in df.columns:
            df = df.sort_values(['zone_name', 'building_type', 'building_id'])
        
        return df
    
    def _convert_timeseries_to_dataframe(self, timeseries_data: List[Dict], options: Dict) -> pd.DataFrame:
        """
        Convertit les donn√©es de s√©ries temporelles en DataFrame optimis√©
        
        Args:
            timeseries_data: Donn√©es des s√©ries temporelles
            options: Options de conversion
            
        Returns:
            pd.DataFrame: DataFrame optimis√©
        """
        df = pd.DataFrame(timeseries_data)
        
        # Conversion et tri par timestamp
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.sort_values(['building_id', 'timestamp'])
        
        # Optimisations des types
        if 'building_id' in df.columns:
            df['building_id'] = df['building_id'].astype('category')
        
        if 'building_type' in df.columns:
            df['building_type'] = df['building_type'].astype('category')
        
        # Optimisation des valeurs num√©riques
        if 'consumption_kwh' in df.columns:
            df['consumption_kwh'] = df['consumption_kwh'].round(4)
        
        if 'temperature_c' in df.columns:
            df['temperature_c'] = df['temperature_c'].round(2)
        
        if 'humidity' in df.columns:
            df['humidity'] = df['humidity'].round(3)
        
        # Optimisation des bool√©ens
        bool_cols = ['is_weekend', 'is_business_hour', 'anomaly_flag']
        for col in bool_cols:
            if col in df.columns:
                df[col] = df[col].astype('bool')
        
        # Optimisation des entiers
        int_cols = ['hour', 'day_of_week', 'month']
        for col in int_cols:
            if col in df.columns:
                df[col] = df[col].astype('int8')
        
        return df
    
    def _check_data_coherence(self, buildings_df: pd.DataFrame, timeseries_df: pd.DataFrame) -> Dict:
        """
        V√©rifie la coh√©rence entre les DataFrames
        
        Args:
            buildings_df: DataFrame des b√¢timents
            timeseries_df: DataFrame des s√©ries temporelles
            
        Returns:
            Dict: R√©sultats de v√©rification
        """
        issues = []
        warnings = []
        
        if buildings_df.empty or timeseries_df.empty:
            return {
                'coherent': False,
                'issues': ['DataFrames vides'],
                'warnings': []
            }
        
        # V√©rification des IDs de b√¢timents
        building_ids_meta = set(buildings_df['building_id'].unique())
        building_ids_ts = set(timeseries_df['building_id'].unique())
        
        missing_in_timeseries = building_ids_meta - building_ids_ts
        orphan_timeseries = building_ids_ts - building_ids_meta
        
        if missing_in_timeseries:
            warnings.append(f"{len(missing_in_timeseries)} b√¢timents sans donn√©es temporelles")
        
        if orphan_timeseries:
            warnings.append(f"{len(orphan_timeseries)} s√©ries temporelles sans b√¢timent")
        
        # V√©rification de la coh√©rence des types
        if 'building_type' in buildings_df.columns and 'building_type' in timeseries_df.columns:
            # Merger pour v√©rifier la coh√©rence
            merged = timeseries_df.merge(
                buildings_df[['building_id', 'building_type']], 
                on='building_id', 
                suffixes=('_ts', '_building')
            )
            
            type_mismatches = (merged['building_type_ts'] != merged['building_type_building']).sum()
            if type_mismatches > 0:
                issues.append(f"{type_mismatches} incoh√©rences de types de b√¢timents")
        
        # V√©rification des donn√©es manquantes critiques
        critical_nulls_buildings = buildings_df[['building_id', 'latitude', 'longitude']].isnull().any(axis=1).sum()
        critical_nulls_timeseries = timeseries_df[['building_id', 'timestamp', 'consumption_kwh']].isnull().any(axis=1).sum()
        
        if critical_nulls_buildings > 0:
            issues.append(f"{critical_nulls_buildings} b√¢timents avec donn√©es critiques manquantes")
        
        if critical_nulls_timeseries > 0:
            issues.append(f"{critical_nulls_timeseries} observations avec donn√©es critiques manquantes")
        
        return {
            'coherent': len(issues) == 0,
            'issues': issues,
            'warnings': warnings,
            'statistics': {
                'buildings_count': len(buildings_df),
                'timeseries_count': len(timeseries_df),
                'unique_buildings_in_timeseries': len(building_ids_ts),
                'coverage_percentage': (len(building_ids_ts & building_ids_meta) / len(building_ids_meta)) * 100 if building_ids_meta else 0
            }
        }
    
    def _apply_data_filters(self, df: pd.DataFrame, data_type: str) -> pd.DataFrame:
        """
        Applique des filtres de nettoyage aux donn√©es
        
        Args:
            df: DataFrame √† filtrer
            data_type: Type de donn√©es ('buildings' ou 'timeseries')
            
        Returns:
            pd.DataFrame: DataFrame filtr√©
        """
        original_count = len(df)
        
        if data_type == 'buildings':
            # Filtres pour les b√¢timents
            if 'latitude' in df.columns and 'longitude' in df.columns:
                # Filtrer les coordonn√©es Malaysia valides
                df = df[
                    (df['latitude'] >= 0.5) & (df['latitude'] <= 7.5) &
                    (df['longitude'] >= 99.0) & (df['longitude'] <= 120.0)
                ]
            
            # Filtrer les consommations n√©gatives
            if 'base_consumption_kwh' in df.columns:
                df = df[df['base_consumption_kwh'] >= 0]
        
        elif data_type == 'timeseries':
            # Filtres pour les s√©ries temporelles
            if 'consumption_kwh' in df.columns:
                # Supprimer les consommations n√©gatives ou extr√™mes
                df = df[
                    (df['consumption_kwh'] >= 0) & 
                    (df['consumption_kwh'] <= 1000)  # Max 1000 kWh par p√©riode
                ]
            
            # Filtrer les temp√©ratures irr√©alistes pour Malaysia
            if 'temperature_c' in df.columns:
                df = df[
                    (df['temperature_c'] >= 15) & 
                    (df['temperature_c'] <= 50)
                ]
            
            # Filtrer l'humidit√© irr√©aliste
            if 'humidity' in df.columns:
                df = df[
                    (df['humidity'] >= 0.2) & 
                    (df['humidity'] <= 1.0)
                ]
        
        filtered_count = len(df)
        if filtered_count < original_count:
            logger.info(f"üßπ Filtrage {data_type}: {original_count} ‚Üí {filtered_count} ({original_count - filtered_count} supprim√©s)")
        
        return df
    
    def _optimize_export_formats(self, requested_formats: List[str], data_size: int) -> List[str]:
        """
        Optimise les formats d'export selon la taille des donn√©es
        
        Args:
            requested_formats: Formats demand√©s
            data_size: Taille des donn√©es
            
        Returns:
            List[str]: Formats optimis√©s
        """
        optimized_formats = []
        
        for format_type in requested_formats:
            if format_type not in EXPORT_CONFIG.SUPPORTED_FORMATS:
                logger.warning(f"‚ö†Ô∏è Format {format_type} non support√©, ignor√©")
                continue
            
            # Optimisations selon la taille
            if format_type == 'xlsx' and data_size > 100000:
                logger.warning("‚ö†Ô∏è Excel non recommand√© pour >100k observations, ajout de CSV")
                optimized_formats.extend(['csv', 'parquet'])
                continue
            
            if format_type == 'json' and data_size > 50000:
                logger.warning("‚ö†Ô∏è JSON non recommand√© pour >50k observations")
                optimized_formats.append('parquet')
                continue
            
            optimized_formats.append(format_type)
        
        # Assurer au moins un format efficace
        if not optimized_formats:
            optimized_formats = ['csv', 'parquet']
        
        return list(set(optimized_formats))  # Supprimer les doublons
    
    def _generate_intelligent_filename(
        self, 
        buildings_data: List[Dict], 
        timeseries_data: List[Dict]
    ) -> str:
        """
        G√©n√®re un nom de fichier intelligent bas√© sur les donn√©es
        
        Args:
            buildings_data: Donn√©es des b√¢timents
            timeseries_data: Donn√©es des s√©ries temporelles
            
        Returns:
            str: Pr√©fixe de fichier intelligent
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Extraction des m√©tadonn√©es
        zones = set()
        building_types = set()
        
        for building in buildings_data:
            if building.get('zone_name'):
                zones.add(building['zone_name'])
            if building.get('building_type'):
                building_types.add(building['building_type'])
        
        # Construction du nom
        name_parts = ['malaysia_electricity']
        
        # Ajouter la zone si unique
        if len(zones) == 1:
            zone_name = list(zones)[0].replace(' ', '_').lower()
            name_parts.append(zone_name)
        elif len(zones) <= 3:
            zone_names = '_'.join([z.replace(' ', '_').lower() for z in sorted(zones)])
            name_parts.append(zone_names)
        else:
            name_parts.append('multi_zones')
        
        # Ajouter le nombre de b√¢timents
        name_parts.append(f"{len(buildings_data)}buildings")
        
        # Ajouter le timestamp
        name_parts.append(timestamp)
        
        return '_'.join(name_parts)
    
    def _enrich_export_result(
        self, 
        export_result: Dict, 
        preparation_result: Dict, 
        manifest_path: str
    ) -> Dict:
        """
        Enrichit le r√©sultat d'export avec des m√©tadonn√©es m√©tier
        
        Args:
            export_result: R√©sultat d'export de base
            preparation_result: R√©sultat de pr√©paration
            manifest_path: Chemin du manifest
            
        Returns:
            Dict: R√©sultat enrichi
        """
        enriched = export_result.copy()
        
        # Ajout d'informations de service
        enriched['service_metadata'] = {
            'export_service_version': '1.0.0',
            'preparation_info': preparation_result.get('preparation_info', {}),
            'manifest_path': manifest_path,
            'usage_recommendations': self._generate_usage_recommendations(export_result),
            'data_quality_assessment': self._assess_export_quality(export_result)
        }
        
        # Ajout de liens de t√©l√©chargement
        enriched['download_links'] = self._generate_download_links(export_result)
        
        # Mise √† jour des statistiques de service
        self.service_statistics['total_files_created'] += len(export_result.get('files_created', {}))
        
        return enriched
    
    def _generate_usage_recommendations(self, export_result: Dict) -> List[str]:
        """
        G√©n√®re des recommandations d'usage pour les fichiers export√©s
        
        Args:
            export_result: R√©sultat d'export
            
        Returns:
            List[str]: Recommandations d'usage
        """
        recommendations = []
        
        files_created = export_result.get('files_created', {})
        total_size_mb = export_result.get('total_size_mb', 0)
        
        # Recommandations par format
        if 'csv' in files_created:
            recommendations.append("Fichiers CSV: Utilisez Excel ou pandas (Python) pour l'analyse")
        
        if 'parquet' in files_created:
            recommendations.append("Fichiers Parquet: Format optimis√© pour Python pandas, Apache Spark")
        
        if 'xlsx' in files_created:
            recommendations.append("Fichier Excel: Ouvrir avec Microsoft Excel, contient onglets multiples")
        
        if 'json' in files_created:
            recommendations.append("Fichiers JSON: Format pour APIs web et applications JavaScript")
        
        # Recommandations par taille
        if total_size_mb > 100:
            recommendations.append("Gros dataset: Utilisez Python pandas ou R pour l'analyse")
        elif total_size_mb < 1:
            recommendations.append("Petit dataset: Compatible avec tous les outils d'analyse")
        
        return recommendations
    
    def _assess_export_quality(self, export_result: Dict) -> Dict:
        """
        √âvalue la qualit√© de l'export r√©alis√©
        
        Args:
            export_result: R√©sultat d'export
            
        Returns:
            Dict: √âvaluation de qualit√©
        """
        quality_score = 100.0
        issues = []
        
        # V√©rification des fichiers cr√©√©s
        files_created = export_result.get('files_created', {})
        
        if not files_created:
            quality_score = 0
            issues.append("Aucun fichier cr√©√©")
        else:
            # V√©rification par format
            for format_type, format_info in files_created.items():
                if not format_info.get('success', True):
                    quality_score -= 20
                    issues.append(f"√âchec export {format_type}")
        
        # V√©rification de la validation des donn√©es
        validation_results = export_result.get('validation_results', {})
        if not validation_results.get('valid', True):
            quality_score -= 30
            issues.extend(validation_results.get('errors', []))
        
        # Bonus pour formats multiples
        if len(files_created) > 1:
            quality_score += 5
        
        return {
            'quality_score': max(0, quality_score),
            'issues': issues,
            'recommendations': [
                "V√©rifiez l'int√©grit√© des fichiers avant utilisation",
                "Consultez le manifest pour les d√©tails techniques"
            ]
        }
    
    def _generate_download_links(self, export_result: Dict) -> Dict:
        """
        G√©n√®re les liens de t√©l√©chargement pour les fichiers
        
        Args:
            export_result: R√©sultat d'export
            
        Returns:
            Dict: Liens de t√©l√©chargement
        """
        download_links = {}
        
        files_created = export_result.get('files_created', {})
        
        for format_type, format_info in files_created.items():
            if format_info.get('success', True):
                format_files = format_info.get('files', {})
                
                download_links[format_type] = {}
                
                for file_type, file_info in format_files.items():
                    if 'path' in file_info:
                        filename = os.path.basename(file_info['path'])
                        download_links[format_type][file_type] = {
                            'filename': filename,
                            'size_mb': file_info.get('size_mb', 0),
                            'download_url': f"/api/download/{filename}"
                        }
        
        return download_links
    
    def get_service_status(self) -> Dict:
        """
        Retourne l'√©tat du service d'export
        
        Returns:
            Dict: √âtat d√©taill√© du service
        """
        uptime = datetime.now() - self.service_statistics['service_start_time']
        
        success_rate = 0
        if self.service_statistics['total_exports'] > 0:
            success_rate = (
                self.service_statistics['successful_exports'] / 
                self.service_statistics['total_exports']
            ) * 100
        
        # V√©rification de l'espace disque
        disk_space_ok = True
        try:
            import shutil
            total, used, free = shutil.disk_usage(APP_CONFIG.EXPORTS_DIR)
            free_gb = free / (1024**3)
            disk_space_ok = free_gb > 1.0  # Au moins 1GB libre
        except:
            free_gb = 0
        
        return {
            'service_name': 'Export Service',
            'status': 'active' if disk_space_ok else 'degraded',
            'uptime_seconds': int(uptime.total_seconds()),
            'disk_space_gb': round(free_gb, 1),
            'statistics': {
                'total_exports': self.service_statistics['total_exports'],
                'successful_exports': self.service_statistics['successful_exports'],
                'success_rate_percent': round(success_rate, 1),
                'total_files_created': self.service_statistics['total_files_created'],
                'total_size_exported_mb': round(self.service_statistics['total_size_exported_mb'], 2)
            }
        }
    
    def get_statistics(self) -> Dict:
        """
        Retourne les statistiques d√©taill√©es du service
        
        Returns:
            Dict: Statistiques compl√®tes
        """
        return self.service_statistics.copy()


# ==============================================================================
# FONCTIONS UTILITAIRES DU SERVICE
# ==============================================================================

def estimate_export_time(data_size: int, formats: List[str]) -> Dict:
    """
    Estime le temps d'export selon la taille et les formats
    
    Args:
        data_size: Nombre d'observations
        formats: Liste des formats
        
    Returns:
        Dict: Estimations de temps
    """
    # Temps de base par format (secondes pour 1000 observations)
    format_times = {
        'csv': 0.5,
        'parquet': 0.3,
        'xlsx': 2.0,
        'json': 1.0
    }
    
    total_time = 0
    format_estimates = {}
    
    for format_type in formats:
        if format_type in format_times:
            format_time = (data_size / 1000) * format_times[format_type]
            format_estimates[format_type] = round(format_time, 1)
            total_time += format_time
    
    return {
        'total_time_seconds': round(total_time, 1),
        'format_breakdown': format_estimates,
        'complexity': 'simple' if total_time < 10 else 'moderate' if total_time < 60 else 'complex'
    }


def validate_export_request(
    buildings_count: int, 
    timeseries_count: int, 
    formats: List[str]
) -> Dict:
    """
    Valide une demande d'export
    
    Args:
        buildings_count: Nombre de b√¢timents
        timeseries_count: Nombre d'observations
        formats: Formats demand√©s
        
    Returns:
        Dict: R√©sultats de validation
    """
    errors = []
    warnings = []
    
    # Validation des tailles
    if buildings_count == 0:
        errors.append("Aucun b√¢timent √† exporter")
    
    if timeseries_count == 0:
        errors.append("Aucune donn√©e temporelle √† exporter")
    
    if timeseries_count > 1000000:
        warnings.append("Dataset tr√®s volumineux - export long")
    
    # Validation des formats
    if not formats:
        errors.append("Aucun format d'export sp√©cifi√©")
    
    unsupported_formats = [f for f in formats if f not in EXPORT_CONFIG.SUPPORTED_FORMATS]
    if unsupported_formats:
        errors.append(f"Formats non support√©s: {unsupported_formats}")
    
    # Recommandations
    if 'xlsx' in formats and timeseries_count > 100000:
        warnings.append("Excel non recommand√© pour >100k observations")
    
    if 'json' in formats and timeseries_count > 50000:
        warnings.append("JSON non recommand√© pour gros datasets")
    
    return {
        'valid': len(errors) == 0,
        'errors': errors,
        'warnings': warnings
    }


# ==============================================================================
# EXEMPLE D'UTILISATION
# ==============================================================================

if __name__ == '__main__':
    # Test du service d'export
    from src.core.data_exporter import DataExporter
    
    # Initialisation
    data_exporter = DataExporter()
    export_service = ExportService(data_exporter)
    
    # Donn√©es de test
    test_buildings = [
        {
            'building_id': 'B001',
            'latitude': 3.15,
            'longitude': 101.7,
            'building_type': 'residential',
            'zone_name': 'kuala_lumpur'
        }
    ]
    
    test_timeseries = [
        {
            'building_id': 'B001',
            'timestamp': '2024-01-01T10:00:00',
            'consumption_kwh': 2.5,
            'temperature_c': 30.0,
            'humidity': 0.8,
            'building_type': 'residential'
        }
    ]
    
    # Test d'export
    result = export_service.export_complete_dataset(
        buildings_data=test_buildings,
        timeseries_data=test_timeseries,
        formats=['csv', 'parquet']
    )
    
    if result['success']:
        print(f"‚úÖ Export test r√©ussi:")
        print(f"üìÅ {len(result['files_created'])} formats cr√©√©s")
        print(f"üíæ {result['total_size_mb']:.2f} MB")
        print(f"üéØ Score qualit√©: {result['service_metadata']['data_quality_assessment']['quality_score']}")
    else:
        print(f"‚ùå Export √©chou√©: {result['error']}")
    
    # Statut du service
    status = export_service.get_service_status()
    print(f"üìä Service: {status['status']}, {status['statistics']['success_rate_percent']}% succ√®s")

                